---
title: "CV"
author: "Yu Dong"
date: "9/17/2018"
output: html_document
---

```{r load package and data}
library(tidyverse)
library(catboost)
library(caret)
library(Metrics)
```


```{r cv}
load('input/new_feature2.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,98)][-flds[[i]],]
    train_pool = catboost.load_pool(
        data = data_train[,-96],
        label = data_train[,96],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))
    
    data_test = all_data[all_data$set== 'train',-c(1,2,98)][flds[[i]],]
    test_pool = catboost.load_pool(
        data = data_test[,-96],
        label = data_test[,96],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 3500,
                   learning_rate = 0.01,
                   random_seed = 42,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    
}

##1
#bestTest = 1.58923673
#bestIteration = 3499

##2
#bestTest = 1.622097489
#bestIteration = 3499

##3
#bestTest = 1.625289618
#bestIteration = 3482

##4
#bestTest = 1.618978338
#bestIteration = 3481

##5
#bestTest = 1.632885051
#bestIteration = 3498

###### bestIteration: 3480 (0.01 lr)
## avg test error = 1.617697

```

CV with feature set 2

```{r cv2}

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,98)][-flds[[i]],]
    train_pool = catboost.load_pool(
        data = data_train[,-96],
        label = data_train[,96],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))
    
    data_test = all_data[all_data$set== 'train',-c(1,2,98)][flds[[i]],]
    test_pool = catboost.load_pool(
        data = data_test[,-96],
        label = data_test[,96],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 4000,
                   learning_rate = 0.01,
                   random_seed = 42,
                   rsm = 0.95,
                   l2_leaf_reg = 3,
                   depth = 8,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    
}

######## rsm

## rsm = 0.95:  ***
###### bestIteration: 3499 (0.01 lr) --- maybe more iter required
## avg test error = 1.617854

## rsm = 0.9:
###### bestIteration: 3499 (0.01 lr)
## avg test error:
## 3500 -> 1.61806484
## 4000 -> 1.61697007


######## l2_leaf_reg

## l2_leaf_reg = 3: ***
## avg test error:
## 3500 -> 1.617854

## l2_leaf_reg = 5:
## avg test error:
## 3500 -> 1.61825532
## 4000 -> 1.617103848


######## depth

## depth = 6:
## avg test error:
## 3500 -> 1.617854

## depth = 8: ****
## avg test error:
## 3500 -> 1.61288864
## 4000 -> 1.61205528

######## border_count

## border_count = 128: **
## avg test error:
## 3500 -> 1.61288864
## 4000 -> 1.61205528

## border_count = 64:
## avg test error:
## 4000 -> 1.61250356

```


CV with feature set 3

```{r cv3}
load('input/new_feature3.RData')


set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,102)][-flds[[i]],]
    train_pool = catboost.load_pool(
        data = data_train[,-100],
        label = data_train[,100],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))
    
    data_test = all_data[all_data$set== 'train',-c(1,2,102)][flds[[i]],]
    test_pool = catboost.load_pool(
        data = data_test[,-100],
        label = data_test[,100],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 3500,
                   learning_rate = 0.01,
                   random_seed = 42,
                   rsm = 0.95,
                   l2_leaf_reg = 3,
                   depth = 8,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')

    submission = data.frame(
        bounces = data_test$bounces,
        actual = data_test[,100],
        prediction = prediction)

    submission$prediction = ifelse(submission$bounces == 1, 0, submission$prediction)
    
    submission$bounces = NULL


    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))

}


## worse than train solely on the unbounced session and combine (see below)

```


CV with feature set 3 and only unbounced sessions

```{r cv unbounced}

load('input/new_feature3.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,102)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-100],
        label = data_train[,100],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))
    
    data_test = all_data[all_data$set== 'train',-c(1,2,102)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-100],
        label = data_test[,100],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 3000,
                   learning_rate = 0.01,
                   random_seed = 42,
                   rsm = 0.95,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')

    submission = data.frame(
        actual = data_test[,100], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,100], 
                       prediction = 0))

    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}

###### rsm

## rsm = 0.95: ***
## RMSE: 1.597480812
## best iter: 2700-3000

## rsm = 0.9
## RMSE: 1.597653274
## best iter: 2900-3000


###### depth

## depth = 8: ***
## RMSE: 1.597480812
## best iter: 2700-3000

## depth = 7:
## RMSE: 1.599652218
## best iter: 3000




```


```{r feature importance}

feature_importance = data.frame(
    feature = names(data_train[,-c(100)]),
    score = catboost.get_feature_importance(model))

```

CV with original feature set and only unbounced sessions (highest LB so far)

```{r cv unbounced}

load('input/cleaned_all_data.RData')
all_data$logRevenue = log(all_data$transactionRevenue + 1)

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data = all_data %>%
    filter(set== 'train') %>%
    select(c(3,6:15, 17:21, 23, 25:29, 31))
    data_train = data[-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-23],
        label = data_train[,23],
        cat_features = c(1:3,5:11,16,17,19,21,22))
    
    data_test = data[flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-23],
        label = data_test[,23],
        cat_features = c(1:3,5:11,16,17,19,21,22))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 2000,
                   learning_rate = 0.005,
                   random_seed = 42,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')

    submission = data.frame(
        actual = data_test[,23], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,23], 
                       prediction = 0))

    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}


## RMSE: 1.653973554

```




```{r cv without new feature set 2 (group avg/max)}

load('input/new_feature3.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,34:97, 102)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-36],
        label = data_train[,36],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))
    
    data_test = all_data[all_data$set== 'train',-c(1,2, 34:97, 102)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-36],
        label = data_test[,36],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 4000,
                   learning_rate = 0.005,
                   random_seed = 42,
                   rsm = 0.95,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')

    submission = data.frame(
        actual = data_test[,36], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,36], 
                       prediction = 0))

    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}

## 3000, 0.01:
## RMSE = 1.601465173
## iter: 2999 (could be more?)


## 4000, 0.005:
## RMSE = 1.603323911
## iter: 3999

```


Train a classification model to predict whether has revenue or not first


```{r cv classfier layer}
load('input/new_feature3.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

all_data$purchaseFlag = ifelse(all_data$transactionRevenue > 0, 1, 0)

for (i in 1:5) {

    print(paste0('start fold ', i, ' training'))
    
    data_train = all_data[all_data$set== 'train',-c(1,2,102)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-c(100,101)],
        label = data_train[,101],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))


    data_test = all_data[all_data$set== 'train',-c(1,2,102)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-c(100,101)],
        label = data_test[,101],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))

    ### modeling

    fit_params <- list(loss_function = 'Logloss',
                       custom_loss = 'F1',
                       iterations = 1500,
                       learning_rate = 0.005,
                       random_seed = 1,
                       rsm = 0.95,
                       l2_leaf_reg = 3,
                       depth = 8,
                       one_hot_max_size = 100,
                       class_weights = c(0.1, 3),
                       train_dir = paste0('train_dir_',i, '_classifier'),
                       verbose = 500)

    print('start classifier training')
    
    model <- catboost.train(
        learn_pool = train_pool, 
        test_pool = test_pool,
        params = fit_params)
    
    print('classifier training ends')

    ### prediction (get the classification score)

    train_predict <- catboost.predict(model, 
                                   train_pool, 
                                   prediction_type = 'RawFormulaVal')

    test_predict <- catboost.predict(model, 
                                   test_pool, 
                                   prediction_type = 'RawFormulaVal')

    data_train$class_pred = train_predict
    data_test$class_pred = test_predict
    
    train_pool = catboost.load_pool(
        data = data_train[,-c(100,101)],
        label = data_train[,100],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))
    
    test_pool = catboost.load_pool(
        data = data_test[,-c(100,101)],
        label = data_test[,100],
        cat_features = c(2,3,5:16,19,24,25,26,28,30,31))

    ### train regression model
    
    print('start regressor training')
    
    fit_params <- list(loss_function = 'RMSE',
                   iterations = 2500,
                   learning_rate = 0.01,
                   random_seed = 42,
                   rsm = 0.9,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir_',i, '_regressor'),
                   verbose = 500)

    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')
    
    print('regressor training ends')

    ## evaluation
    
    submission = data.frame(
        actual = data_test[,100], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,100], 
                       prediction = 0))

    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}

####### class_weights

## class_weights = c(0.1, 4)  
## classifier:
## average logloss: 0.166987118 (1500-2000)
## final local cv: 1.597312486 (3000)


## class_weights = c(0.1, 5)
## classifier: 
## average logloss: 0.162059522 (1200-1800)
## final local cv: 1.597624908 (3000)


## class_weights = c(0.1, 3) ****
## classifier: 
## average logloss: 0.170210266 (1600-2000)
## final local cv: 1.597267473 (3000)


## class_weights = c(0.1, 1) 
## classifier: 
## average logloss: 0.148226361 (2000)
## final local cv: 1.598508803 (600-3000)


####### iters and lr

## classifier iters = 2000, lr = 0.005
## regressor iters = 3000, lr = 0.005
## classifier: 
## average logloss: 0.170210266 (1600-2000)
## final local cv: 1.597267473 (3000)


## classifier iters = 1500, lr = 0.005   *****
## regressor iters = 3000, lr = 0.01
## classifier: 
## average logloss: 0.170486955 (1500)
## final local cv: 1.5955064 (1700 - 2600)



## make rounds smaller for classifier // more rounds for regressor
## only keep 0/1 pred
## optimize precision / recall



```

```{r featureset 4}

load('input/new_feature4.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

all_data$purchaseFlag = ifelse(all_data$transactionRevenue > 0, 1, 0)

for (i in 1:5) {

    print(paste0('start fold ', i, ' training'))
    
    data_train = all_data[all_data$set== 'train',-c(1,2,100)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-c(98,99)],
        label = data_train[,99],
        cat_features = c(1:19))


    data_test = all_data[all_data$set== 'train',-c(1,2,100)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-c(98,99)],
        label = data_test[,99],
        cat_features = c(1:19))

    ### modeling

    fit_params <- list(loss_function = 'Logloss',
                       custom_loss = 'F1',
                       iterations = 2000,
                       learning_rate = 0.005,
                       random_seed = 1,
                       rsm = 0.95,
                       l2_leaf_reg = 3,
                       depth = 8,
                       #one_hot_max_size = 100,
                       class_weights = c(0.1, 3),
                       train_dir = paste0('train_dir_',i, '_classifier'),
                       verbose = 500)

    print('start classifier training')
    
    model <- catboost.train(
        learn_pool = train_pool, 
        test_pool = test_pool,
        params = fit_params)
    
    print('classifier training ends')

    ### prediction (get the classification score)

    train_predict <- catboost.predict(model, 
                                   train_pool, 
                                   prediction_type = 'RawFormulaVal')

    test_predict <- catboost.predict(model, 
                                   test_pool, 
                                   prediction_type = 'RawFormulaVal')

    data_train$class_pred = train_predict
    data_test$class_pred = test_predict
    
    train_pool = catboost.load_pool(
        data = data_train[,-c(98,99)],
        label = data_train[,98],
        cat_features = c(1:19))
    
    test_pool = catboost.load_pool(
        data = data_test[,-c(98,99)],
        label = data_test[,98],
        cat_features = c(1:19))

    ### train regression model
    
    print('start regressor training')
    
    fit_params <- list(loss_function = 'RMSE',
                   iterations = 2500,
                   learning_rate = 0.005,
                   random_seed = 42,
                   rsm = 0.9,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir_',i, '_regressor'),
                   verbose = 500)

    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')
    
    print('regressor training ends')

    ## evaluation
    
    submission = data.frame(
        actual = data_test[,98], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,98], 
                       prediction = 0))

    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}


```

Feature set 5 unbounced sessions regression

```{r feature set 5 unbounced}

load('input/new_feature5.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,118)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-116],
        label = data_train[,116],
        cat_features = c(1:17))
    
    data_test = all_data[all_data$set== 'train',-c(1,2,118)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-116],
        label = data_test[,116],
        cat_features = c(1:17))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 5000,
                   learning_rate = 0.01,
                   random_seed = 42,
                   rsm = 0.95,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')

    submission = data.frame(
        actual = data_test[,116], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,116], 
                       prediction = 0))
    
    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}

### rsm = 0.95, lr = 0.005

## 3500: 1.600288506
## 5000: 1.59881076

### lr = 0.01

## 4500: 1.597657454


```



Feature set 6 unbounced sessions regression

```{r feature set 6 unbounced}

load('input/new_feature6.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,93)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-91],
        label = data_train[,91],
        cat_features = c(1:15))
    
    data_test = all_data[all_data$set== 'train',-c(1,2,93)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-91],
        label = data_test[,91],
        cat_features = c(1:15))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 4500,
                   learning_rate = 0.01,
                   random_seed = 42,
                   rsm = 0.95,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')

    submission = data.frame(
        actual = data_test[,91], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,91], 
                       prediction = 0))
    
    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}


### lr = 0.01

## 4500: 1.597976369


```



```{r feature set 5 unbounced2}

load('input/new_feature5.RData')

all_data$month = as.factor(all_data$month)
all_data$hour = as.factor(all_data$hour)

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

for (i in 1:5) {
    data_train = all_data[all_data$set== 'train',-c(1,2,118)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-116],
        label = data_train[,116],
        cat_features = c(1:19))
    
    data_test = all_data[all_data$set== 'train',-c(1,2,118)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-116],
        label = data_test[,116],
        cat_features = c(1:19))

    fit_params <- list(loss_function = 'RMSE',
                   iterations = 4500,
                   learning_rate = 0.01,
                   random_seed = 42,
                   rsm = 0.95,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir',i),
                   verbose = 500)

    print(paste0('start fold ', i, ' training'))
    
    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')

    submission = data.frame(
        actual = data_test[,116], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,116], 
                       prediction = 0))
    
    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}


### lr = 0.01

## 4500: 1.598295349 (~4300)


```



```{r featureset 5 classifier}

load('input/new_feature5.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

all_data$purchaseFlag = ifelse(all_data$transactionRevenue > 0, 1, 0)

for (i in 1:5) {

    print(paste0('start fold ', i, ' training'))
    
    data_train = all_data[all_data$set== 'train',-c(1,2,118)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-c(116,117)],
        label = data_train[,117],
        cat_features = c(1:17))


    data_test = all_data[all_data$set== 'train',-c(1,2,118)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-c(116,117)],
        label = data_test[,117],
        cat_features = c(1:17))

    ### modeling

    fit_params <- list(loss_function = 'Logloss',
                       custom_loss = 'F1',
                       iterations = 2000,
                       learning_rate = 0.005,
                       random_seed = 1,
                       rsm = 0.95,
                       l2_leaf_reg = 3,
                       depth = 8,
                       #one_hot_max_size = 100,
                       class_weights = c(0.1, 3),
                       train_dir = paste0('train_dir_',i, '_classifier'),
                       verbose = 500)

    print('start classifier training')
    
    model <- catboost.train(
        learn_pool = train_pool, 
        test_pool = test_pool,
        params = fit_params)
    
    print('classifier training ends')

    ### prediction (get the classification score)

    train_predict <- catboost.predict(model, 
                                   train_pool, 
                                   prediction_type = 'RawFormulaVal')

    test_predict <- catboost.predict(model, 
                                   test_pool, 
                                   prediction_type = 'RawFormulaVal')

    data_train$class_pred = train_predict
    data_test$class_pred = test_predict
    
    train_pool = catboost.load_pool(
        data = data_train[,-c(116,117)],
        label = data_train[,116],
        cat_features = c(1:17))
    
    test_pool = catboost.load_pool(
        data = data_test[,-c(116,117)],
        label = data_test[,116],
        cat_features = c(1:17))

    ### train regression model
    
    print('start regressor training')
    
    fit_params <- list(loss_function = 'RMSE',
                   iterations = 2500,
                   learning_rate = 0.005,
                   random_seed = 42,
                   rsm = 0.9,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir_',i, '_regressor'),
                   verbose = 500)

    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')
    
    print('regressor training ends')

    ## evaluation
    
    submission = data.frame(
        actual = data_test[,116], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,116], 
                       prediction = 0))

    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}

## classification target: logloss
### 1.596428452


```



```{r featureset 7 classifier}

load('input/new_feature7.RData')

set.seed(42)
flds <- createFolds(1:903653, k = 5, list = TRUE, returnTrain = FALSE)

all_data$purchaseFlag = ifelse(all_data$transactionRevenue > 0, 1, 0)

for (i in 1:5) {

    print(paste0('start fold ', i, ' training'))
    
    data_train = all_data[all_data$set== 'train',-c(1,2,122)][-flds[[i]],]
    data_train = data_train %>% filter(bounces == 0)
    train_pool = catboost.load_pool(
        data = data_train[,-c(120,121)],
        label = data_train[,121],
        cat_features = c(1:19))


    data_test = all_data[all_data$set== 'train',-c(1,2,122)][flds[[i]],]
    data_test_bounced = data_test %>% filter(bounces == 1)
    data_test = data_test %>% filter(bounces == 0)
    test_pool = catboost.load_pool(
        data = data_test[,-c(120,121)],
        label = data_test[,121],
        cat_features = c(1:19))

    ### modeling

    fit_params <- list(loss_function = 'Logloss',
                       custom_loss = 'F1',
                       iterations = 2000,
                       learning_rate = 0.005,
                       random_seed = 1,
                       rsm = 0.95,
                       l2_leaf_reg = 3,
                       depth = 8,
                       #one_hot_max_size = 100,
                       class_weights = c(0.1, 3),
                       train_dir = paste0('train_dir_',i, '_classifier'),
                       verbose = 500)

    print('start classifier training')
    
    model <- catboost.train(
        learn_pool = train_pool, 
        test_pool = test_pool,
        params = fit_params)
    
    print('classifier training ends')

    ### prediction (get the classification score)

    train_predict <- catboost.predict(model, 
                                   train_pool, 
                                   prediction_type = 'RawFormulaVal')

    test_predict <- catboost.predict(model, 
                                   test_pool, 
                                   prediction_type = 'RawFormulaVal')

    data_train$class_pred = train_predict
    data_test$class_pred = test_predict
    
    train_pool = catboost.load_pool(
        data = data_train[,-c(120,121)],
        label = data_train[,120],
        cat_features = c(1:19))
    
    test_pool = catboost.load_pool(
        data = data_test[,-c(120,121)],
        label = data_test[,120],
        cat_features = c(1:19))

    ### train regression model
    
    print('start regressor training')
    
    fit_params <- list(loss_function = 'RMSE',
                   iterations = 2500,
                   learning_rate = 0.005,
                   random_seed = 42,
                   rsm = 0.9,
                   l2_leaf_reg = 3,
                   depth = 7,
                   border_count = 64,
                   one_hot_max_size = 100,
                   train_dir = paste0('train_dir_',i, '_regressor'),
                   verbose = 500)

    model <- catboost.train(
        learn_pool = train_pool,
        test_pool = test_pool,
        params = fit_params)

    prediction <- catboost.predict(model, 
                               test_pool, 
                               prediction_type = 'RawFormulaVal')
    
    print('regressor training ends')

    ## evaluation
    
    submission = data.frame(
        actual = data_test[,120], 
        prediction = prediction)

    submission = rbind(submission, 
                       data.frame(
                       actual = data_test_bounced[,120], 
                       prediction = 0))

    print(paste0('RMSE is ', rmse(submission$actual, submission$prediction)))
    
}

## classification target: logloss
### 1.596635058


```

